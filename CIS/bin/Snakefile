#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 21 15:43:06 2021

@author: cfos
"""
###########
# Libraries
###########

import os
import glob
import re
from datetime import date
import pandas as pd
import yaml
import subprocess
import glob
import pandas as pd

#################
# Custom functions
#################

def find_input_data(READSDIR, data_type, SUFFIX):
    reads = glob.glob(os.path.join(READSDIR,'*'+SUFFIX))
    main_exclusion = ['Undet','NEG_', 'NC_']
    neg_inclusion = ['NEG_', 'NC_']
    if data_type == "main":
        reads = [i for i in reads if not any(b in i for b in main_exclusion)]
    elif data_type == "neg":
        reads = [i for i in reads if any(b in i for b in neg_inclusion)]
    result = []
    for read in reads:
        if SUFFIX == '_L001_R1_001.fastq.gz':
            sample = re.sub("_S\d+_L001.*", repl = "", string=os.path.basename(read))
            barcode = re.search("S\d+_L001.*", string=os.path.basename(read)).group()
            barcode = re.sub("_.*",repl="",string=barcode)
            res = {'sample':sample,'barcode':barcode, 'fq1':read,'fq2':read.replace("_R1","_R2")}
        else:
            sample = re.sub(SUFFIX, repl = "", string=os.path.basename(read))
            barcode = "N/A"
            res = {'sample':sample,'barcode':barcode, 'fq1':read,'fq2':read.replace("_R1","_R2")}    
        result.append(res)
    result =  pd.DataFrame(result, dtype=str).set_index(["sample"], drop=False)
    return result


def get_trim_names(wildcards):
    """
    This function:
      1. Returns the correct input and output trimmed file names for fastp.
    """
    inFile = INPUT_TABLE.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()
    return "--in1 " + inFile[0] + " --in2 " + inFile[1] + " --out1 " + os.path.join(RESULT_DIR, wildcards.sample, "fastp", wildcards.sample + "_trimmed_R1.fq.gz") + " --out2 " + os.path.join(RESULT_DIR, wildcards.sample, "fastp", wildcards.sample + "_trimmed_R2.fq.gz")

def get_fastq(wildcards):
    """This function returns the forward and reverse fastq files for samples"""
    return INPUT_TABLE.loc[(wildcards.sample), ["fq1", "fq2"]].dropna()


###############
# Configuration
###############

READSDIR = config["reads_dir"]
RESULT_DIR = config["outdir"]
REFERENCE=config["reference"]
SCHEME=config["scheme"]
COVERAGE_SCRIPT=config["coverage_script"]
CONVERTER = config["converter_script"]
VCF_MOD = config["vcf_script"]
ISOLATES = config['isolates']
VARIANT_PROGRAM = os.path.basename(config['variant_program'])
SCHEME_NAME = os.path.basename(SCHEME)
KRAKEN2_DB = config["kraken2_db"]
CON_FREQ = config["consensus_freq"]

SUFFIX = config['suffix']
SUFFIX = SUFFIX.replace('R2','R1')

MAIN_TABLE = find_input_data(READSDIR, "main", SUFFIX)
MAIN_SAMPLES = MAIN_TABLE.index.get_level_values('sample').unique().tolist()
try:
    NEG_TABLE = find_input_data(READSDIR, "neg", SUFFIX)
    NEG_SAMPLES = NEG_TABLE.index.get_level_values('sample').unique().tolist()
except:
    print("\nNo negative control samples detected\n")
    NEG_SAMPLES = []    
INPUT_TABLE = find_input_data(READSDIR, "all", SUFFIX)

if ISOLATES != False:
    MAIN_SAMPLES = [i for i in MAIN_SAMPLES if any(b in i for b in ISOLATES)]
    NEG_SAMPLES = [i for i in NEG_SAMPLES if any(b in i for b in ISOLATES)]

config['analysis_samples'] = '; '.join(MAIN_SAMPLES)
config['neg_samples'] = '; '.join(NEG_SAMPLES)

if SCHEME_NAME == 'swift.bed':
    IVAR_OFFSET = 5
else:
    IVAR_OFFSET = 0

if not os.path.isfile(REFERENCE+'.fai'):
    os.system("samtools faidx {} 2> /dev/null".format(REFERENCE))
if not os.path.isfile(REFERENCE+'.bwt'):
    os.system("bwa index {} 2> /dev/null".format(REFERENCE))

TODAY=date.today().strftime("%Y-%m-%d")

################
# Optional removal of trimmed reads (to save space)
################
if args.keep_reads == False:
    onsuccess:
        print("Removing trimmed reads (input reads remain untouched)\n")
        dead_reads = glob.glob(outdir + '/**/*trimmed*.gz', recursive=True)
        [os.remove(x) for x in dead_reads]


################
# Desired outputs
################

rule final_qc:
    input:
        snpEff_file  = expand(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.annotated.vcf"), sample = MAIN_SAMPLES),
        coverage_plot = expand(os.path.join(RESULT_DIR, "{sample}/coverage/{sample}.coverage_plots.pdf"), sample = MAIN_SAMPLES),
        neg_reports = expand(os.path.join(RESULT_DIR, "{sample}.qc_results.csv"), sample = NEG_SAMPLES),
        sample_reports = expand(os.path.join(RESULT_DIR, "{sample}.qc_results.csv"), sample = MAIN_SAMPLES)
    message:
        "Pipeline complete!"
    run:
        qc_files = glob.glob(RESULT_DIR+'/*qc_results.csv')
        combined_csv = pd.concat([pd.read_csv(f) for f in qc_files ])
        combined_csv.to_csv( os.path.join(RESULT_DIR,TODAY+"_QC.csv"), index=False, header=True)
        with open(os.path.join(RESULT_DIR,"config.yaml"), 'w') as outfile:
            yaml.dump(config, outfile, default_flow_style=False)


########################
# Rules - common
########################

rule fastp:
    input:
        get_fastq,
    output:
        fq1  = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R1.fq.gz"),
        fq2  = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R2.fq.gz"),
        html = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_fastp.html"),
        json = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_fastp.json")
    message:"trimming {wildcards.sample} reads"
    threads: 4
    log:
        os.path.join(RESULT_DIR,"{sample}/fastp/{sample}.log.txt")
    params:
        in_and_out_files =  get_trim_names,
        sampleName = "{sample}",
        qualified_quality_phred = 20,
        cut_mean_quality = 20,
        unqualified_percent_limit = 10,
        length_required = 50
    resources: cpus=4
    shell:
        """
        touch {output.fq2}
        fastp --thread {threads} \
        --detect_adapter_for_pe \
        --cut_front \
        --cut_tail \
        --trim_poly_x \
        --cut_mean_quality 20 \
        --qualified_quality_phred {params.qualified_quality_phred} \
        --unqualified_percent_limit {params.unqualified_percent_limit} \
        --correction \
        --length_required 50 \
        --html {output.html} \
        --json {output.json} \
        {params.in_and_out_files} \
        2>{log}
        """

########################
# Rules - main samples
########################

rule bwa_map_sort:
    input:
        r1 = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R1.fq.gz"),
        r2 = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R2.fq.gz")
    output:
        bam  = os.path.join(RESULT_DIR, "{sample}/{sample}.sorted.bam")
    message:"mapping {wildcards.sample} reads to reference"
    threads: 4
    log:
        os.path.join(RESULT_DIR,"{sample}/{sample}.bwa.log")
    params:
        reference = REFERENCE
    resources: cpus=4
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    shell:
        """
        bwa mem -t {threads} {params.reference} \
        {input.r1} {input.r2} 2> {log} | \
        samtools sort -@ {threads} 2> {log} | \
        samtools view -@ {threads} -F 4 --write-index -o {output.bam} 2> /dev/null
        samtools index {output.bam}
        """

rule ivar_trim:
    input:
        bam  = os.path.join(RESULT_DIR, "{sample}/{sample}.sorted.bam")
    output:
        sort_trim_bam  = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    message:"mapping {wildcards.sample} reads to reference"
    threads: 4
    log:
        os.path.join(RESULT_DIR,"{sample}/ivar/{sample}.ivar_trim.log")
    params:
        trim_bam  = temp(os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.bam")),
        bedfile = SCHEME,
        offset = IVAR_OFFSET,
        prefix = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim")
    resources: cpus=4
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    shell:
        """
        touch {output.sort_trim_bam}
        ivar trim -i {input.bam} -x {params.offset} -b {params.bedfile} -p {params.prefix} -e 2&> {log}
        samtools sort -@ {threads} --write-index {params.trim_bam} -o {output.sort_trim_bam} 2> /dev/null
        samtools index {output.sort_trim_bam}
        """

rule genomecov:
    input:
        bam  = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    output:
        main_plot  = os.path.join(RESULT_DIR, "{sample}/coverage/{sample}.coverage_plots.pdf"),
        amp_plot  = os.path.join(RESULT_DIR, "{sample}/coverage/{sample}.check_amplicons.pdf")
    message:"getting genome coverage statistics for {wildcards.sample}"
    threads: 1
    params:
        coverage  = temp(os.path.join(RESULT_DIR, "{sample}/coverage/{sample}.coverage.txt")),
        script = COVERAGE_SCRIPT,
        bedfile = SCHEME,
        scheme = SCHEME_NAME,
    resources: cpus=1
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    shell:
        """
        touch {output.amp_plot}
        bedtools genomecov -ibam {input.bam} -d > {params.coverage}  2> /dev/null
        Rscript {params.script} {params.coverage} {params.bedfile} {output.main_plot} {output.amp_plot} {params.scheme}  2> /dev/null
        """

rule qualimap:
    input:
        bam  = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    output:
        report  = os.path.join(RESULT_DIR, "{sample}/bamqc/genome_results.txt"),
    message:"getting bamQC metrics for {wildcards.sample}"
    threads: 1
    params:
        outdir = os.path.join(RESULT_DIR, "{sample}/bamqc")
    resources: cpus=1
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    run:
        cmd = "qualimap bamqc -bam {0} -nt {1} -outdir {2} 2&> /dev/null".format(input.bam, threads,params.outdir)
        p = subprocess.Popen(cmd, shell= True, stdout= subprocess.PIPE, stderr= subprocess.PIPE)
        stdout, stderr= p.communicate()
        if 'Failed to run bamqc' in str(stderr):
            res = ['BamQC report','There is a 0% of reference with a coverageData >= 10X',
            'number of reads = 0',
            'number of mapped bases = 0 bp',
            'NC_045512.2	29903	0	0	0']
            if not os.path.exists(params.outdir):
                os.makedirs(params.outdir)
            with open(output.report,'w') as f:
                f.write("\n".join(map(str, res)))

rule samtools_mpileup:
    input:
        bam  = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    output:
        mpileup  = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.mpileup"))
    message:"generating mpileup for {wildcards.sample}"
    threads: 4
    log:
        os.path.join(RESULT_DIR,"{sample}/ivar/{sample}.mpileup.log")
    params:
        reference = REFERENCE
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    resources: cpus=4
    shell:
        """
        samtools mpileup -A -d 0 -B -Q 0 --reference {params.reference} {input.bam} -o {output.mpileup} 2>{log}
        """

rule ivar_variants:
    input:
        mpileup  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.mpileup")
    output:
        vcf_file  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.ivar.vcf.gz"),
        temp_vcf_file = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp.vcf")),
        temp_vcf_file2 = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp.vcf.gz"))
    message:"calling variants for {wildcards.sample}"
    threads: 4
    log:
        os.path.join(RESULT_DIR,"{sample}/variants/{sample}.ivar_variants.log")
    params:
        tsv_file  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.ivar.tsv"),
        bedfile = SCHEME,
        prefix = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.ivar"),
        reference = REFERENCE,
        converter = CONVERTER,
        qual = 20,
        depth = 10,
        con_freq = CON_FREQ,
        snv_freq = 0.1
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    resources: cpus=4
    shell:
        """
        cat {input.mpileup} | ivar variants -p {params.prefix} -r {params.reference} -q {params.qual} -m {params.depth} -t {params.snv_freq} 2&>{log}
        python {params.converter} {params.tsv_file} {output.temp_vcf_file} 2> {log}
        bgzip -c {output.temp_vcf_file} > {output.temp_vcf_file2}
        bcftools index {output.temp_vcf_file2}
        bcftools norm -Ou -a -m - {output.temp_vcf_file2} 2> {log} | \
        bcftools view -i "FORMAT/ALT_FREQ >= {params.snv_freq} & FORMAT/ALT_QUAL >= 20 & INFO/DP >= 10" -Oz -o {output.vcf_file} {output.temp_vcf_file2}
        bcftools index {output.vcf_file}
        bcftools +setGT {output.vcf_file} -- -t q -i 'GT="1" && FORMAT/ALT_FREQ < {params.con_freq} & INFO/DP >= 10' -n 'c:0/1' 2> {log}| \
        bcftools +setGT -- -t q -i 'TYPE="indel" && INFO/AF < {params.con_freq}' -n . 2>> {log} | \
        bcftools +setGT -o {output.vcf_file} -- -t q -i 'GT="1" && FORMAT/ALT_FREQ >= {params.con_freq} & INFO/DP >= 10' -n 'c:1/1' 2> {log}
        bcftools index -f {output.vcf_file}
        """

rule lofreq_variants:
    input:
        bam = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    output:
        vcf_file  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.lofreq.vcf.gz"),
        new_bam = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp.bam")),
        new_bam_index = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp.bam.bai")),
        tmp_vcf1 = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp1.vcf.gz")),
        tmp_vcf2 = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp2.vcf.gz")),
        tmp_vcf1_index = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp1.vcf.gz.tbi")),
        tmp_vcf2_index = temp(os.path.join(RESULT_DIR, "{sample}/variants/{sample}.tmp2.vcf.gz.tbi"))
    message:"calling variants for {wildcards.sample}"
    threads: 8
    log:
        os.path.join(RESULT_DIR,"{sample}/variants/{sample}.lofreq.log")
    params:
        vcf_script = VCF_MOD,
        prefix = "{sample}",
        reference = REFERENCE,
        depth = 10,
        snv_freq = 0.1,
        con_freq = CON_FREQ
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    resources: cpus=8
    shell:
        """
        lofreq indelqual --dindel {input.bam} -f {params.reference} | \
        samtools sort -@ {threads} -o {output.new_bam} 2> /dev/null
        samtools index {output.new_bam}
        lofreq call-parallel --no-baq --call-indels --pp-threads {threads} \
        -f {params.reference} -o {output.tmp_vcf1} {output.new_bam} 2> {log}
        bash {params.vcf_script} -i {output.tmp_vcf1} -g 1 -o {output.tmp_vcf2}
        bcftools +fill-tags {output.tmp_vcf2} -Ou -- -t "TYPE" | \
        bcftools norm -Ou -a -m -  2> /dev/null | \
        bcftools view -f 'PASS,.' -i "INFO/AF >= {params.snv_freq}" -Oz -o {output.vcf_file}
        bcftools index {output.vcf_file}
        bcftools +setGT {output.vcf_file} -- -t q -i 'GT="1" && INFO/AF < {params.con_freq}' -n 'c:0/1' 2>> {log} | \
        bcftools +setGT -- -t q -i 'TYPE="indel" && INFO/AF < {params.con_freq}' -n . 2>> {log} | \
        bcftools +setGT -o {output.vcf_file} -- -t q -i 'GT="1" && INFO/AF >= {params.con_freq}' -n 'c:1/1' 2>> {log}
        bcftools index -f {output.vcf_file}
        """

if VARIANT_PROGRAM == 'ivar':
    proper_vcf = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.ivar.vcf.gz")
elif VARIANT_PROGRAM == 'lofreq':
    proper_vcf = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.lofreq.vcf.gz")

rule generate_consensus:
    input:
        vcf_file  = proper_vcf,
        bam = os.path.join(RESULT_DIR, "{sample}/ivar/{sample}.primertrim.sorted.bam")
    output:
        consensus  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.consensus.fa"),
        mask = temp(os.path.join(RESULT_DIR, "{sample}/variants/mask.bed")),
        variants_bed = temp(os.path.join(RESULT_DIR, "{sample}/variants/variants.bed"))
    message:"calling a consensus for {wildcards.sample}"
    threads: 1
    log:
        os.path.join(RESULT_DIR,"{sample}/variants/{sample}.consensus.log")
    params:
        prefix = "{sample}",
        reference = REFERENCE,
        freq = CON_FREQ
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    resources: cpus=1
    shell:
        """
        bcftools query -f'%CHROM\t%POS0\t%END\n' {input.vcf_file} > {output.variants_bed}
        bedtools genomecov -bga -ibam {input.bam} | awk '$4 < 10' | \
        bedtools subtract -a - -b {output.variants_bed} > {output.mask}
        bcftools consensus -p {params.prefix} -f {params.reference} --mark-del '-' -m {output.mask} -H I -i 'INFO/DP >= 10 & GT!="mis"' {input.vcf_file} 2> {log} | \
        sed "/^>/s/{params.prefix}.*/{params.prefix}/" > {output.consensus}
        """

rule snpeff:
    input:
        vcf_file  = proper_vcf
    output:
        vcf_file  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.annotated.vcf")
    message:"annotating variants for {wildcards.sample}"
    threads: 1
    params:
        prefix = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.snpeff")
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    resources: cpus=1
    shell:
        """
        snpEff eff -csvStats {params.prefix}.stats.csv -s {params.prefix}.stats.html NC_045512.2 {input.vcf_file} > {output.vcf_file}
        """

rule pangolin:
    input:
        fasta  = os.path.join(RESULT_DIR, "{sample}/variants/{sample}.consensus.fa")
    output:
        report = os.path.join(RESULT_DIR, "{sample}/pangolin/{sample}.lineage_report.csv")
    shell:
        """
        eval "$(conda shell.bash hook)" && conda activate pangolin && pangolin --outfile {output.report} {input.fasta} &> /dev/null
        """

rule sample_qc:
    input:
        bamqc  = os.path.join(RESULT_DIR, "{sample}/bamqc/genome_results.txt"),
        lineages =  os.path.join(RESULT_DIR, "{sample}/pangolin/{sample}.lineage_report.csv")
    output:
        report = temp(os.path.join(RESULT_DIR, "{sample}.qc_results.csv"))
    params:
        sample = "{sample}"
    wildcard_constraints:
        sample="(?!NC)(?!NEG).*"
    run:
        lineages = pd.read_csv(input.lineages)
        lineages.loc[0,'taxon'] = params.sample
        qc = pd.read_csv(input.bamqc, sep="\n")
        df=pd.DataFrame(columns=['ID','NoReads','NoBases','%Ref','Mean Cov','std','pangolin lineage', 'scorpio_call','QC'])

        ref=qc[qc['BamQC report'].str.contains('coverageData >= 10X')].iloc[0,0].split()[3]
        No_reads=int(qc[qc['BamQC report'].str.contains('number of reads')].iloc[0].tolist()[0].split()[-1].replace(',',''))
        No_bases=int(qc[qc['BamQC report'].str.contains('number of mapped bases')].iloc[0].tolist()[0].split()[-2].replace(',',''))
        mean_cov=int(float(qc.tail(n=1).iloc[0][0].split()[3]))
        std=int(float(qc.tail(n=1).iloc[0][0].split()[4]))
        lineage = lineages.loc[0,'lineage']
        scorpio_call = lineages.loc[0,'scorpio_call']
        if int(float(ref.split('%')[0]))>79:
            df.loc[0]=[params.sample,No_reads,No_bases,ref,mean_cov,std,lineage,scorpio_call,'PASS']
        else:
            df.loc[0]=[params.sample,No_reads,No_bases,ref,mean_cov,std,lineage,scorpio_call,'FAIL']
        df.to_csv(output.report, header=True,index=False)


########################
# Rules - negative controls
########################

rule kraken2:
    input:
        forward_reads = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R1.fq.gz"),
        reverse_reads = os.path.join(RESULT_DIR, "{sample}/fastp/{sample}_trimmed_R2.fq.gz")
    output:
        result = os.path.join(RESULT_DIR, "{sample}/kraken2/{sample}.kraken_result.txt"),
        report = os.path.join(RESULT_DIR, "{sample}/kraken2/{sample}.kraken_report.txt")
    log:
        os.path.join(RESULT_DIR, "{sample}/kraken2/{sample}.kraken2.log")
    params:
        KRAKEN2_DB = KRAKEN2_DB
    wildcard_constraints:
        sample = "(NC|NEG).*"
    threads: 4
    resources:
        mem_mb=55000,
        cpus=4
    shell:
        """
        touch '{output.result}'
        touch '{output.report}'
        kraken2 --threads 4 \
        --gzip-compressed \
        '{input.forward_reads}' '{input.reverse_reads}' \
        --output '{output.result}' \
        --report '{output.report}' \
        --db '{params.KRAKEN2_DB}' \
        2>{log}
        """

rule neg_qc:
    input:
        kraken = os.path.join(RESULT_DIR, "{sample}/kraken2/{sample}.kraken_result.txt")
    output:
        report = temp(os.path.join(RESULT_DIR, "{sample}.qc_results.csv"))
    params:
        today = TODAY,
        sample = "{sample}"
    wildcard_constraints:
        sample = "(NC|NEG).*"
    run:
        df=pd.DataFrame(columns=['ID','NoReads','NoBases','%Ref','Mean Cov','std','pangolin lineage', 'scorpio_call','QC'])
        try:
            neg=pd.read_csv(input.kraken, sep='\t', header=None)
            if sum(neg[3]) > 10000:
                df.loc[0]=[params.sample,neg.shape[0],sum(neg[3]),'-','-','-','-','-','FAIL']
            else:
                df.loc[0]=[params.sample,neg.shape[0],sum(neg[3]),'-','-','-','-','-','PASS']
        except:
            df.loc[0]=[params.sample,'No_reads',0,'-','-','-','-','-','PASS']
        df.to_csv(output.report, header=True,index=False)
